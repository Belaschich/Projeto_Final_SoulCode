# Projeto_Final_SoulCode
 **REQUISITOS OBRIGATÓRIOS**
 

 - [x]  Obrigatoriamente os datasets devem ter formatos diferentes (CSV / Json / Parquet / Sql / NoSql) e 1 deles obrigatoriamente tem que ser em CSV.
 
 - [X] Operações com Pandas (limpezas , transformações e normalizações) 
 
 - [X] Operações usando PySpark com a descrição de cada uma das operações. 
 
 - [X] Operações utilizando o SparkSQL com a descrição de cada umas das operações. 
 
 - [X] Os datasets utilizados podem ser em lingua estrangeira , mas devem ao final terem seus dados/colunas exibidos na lingua PT-BR.
 
 - [X] Os datasets devem ser salvos e operados em armazenamento cloud obrigatoriamente dentro da plataforma GCP (não pode ser usado Google drive ou armazenamento alheio ao google) 
 
 - [X] Os dados tratados devem ser armazenados também em GCP, mas obrigatoriamente em um datalake(Gstorage ) , DW(BigQuery) ou em ambos. 
 
 - [X] Deve ser feito análises dentro do Big Query utilizando a linguagem padrão SQL com a descrição das consultas feitas. 
 
 - [X] Deve ser criado no datastudio um dash board simples para exibição gráfica dos dados tratados trazendo insights importantes
  
 -  [X] E deve ser demonstrado em um workflow simples (gráfico) as etapas de ETL.
 
 
   **REQUISITOS DESEJÁVEIS**
   

 
 - [X] Implementar captura e ingestão de dados por meio de uma PIPELINE com modelo criado em apache beam usando o dataflow para o work 
  
 - [X] Criar plotagens usando pandas para alguns insights durante o processo de Transformação
  
 - [X] Por meio de uma PIPELINE fazer o carregamento dos dados normalizados diretamente para um DW ou DataLake ou ambos
  
 - [X] Montar um relatório completo com os insights que justificam todo o processo de ETL utilizado.
